{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Overview \n",
        "\n",
        "The notebook is based on the **official Colab notebook from the NeRFmm authors** , with **minor modifications** to the pre- and post-processing steps to better fit our specific reconstruction needs.\n",
        "\n",
        "You can get the original version here : https://colab.research.google.com/drive/1pRljG5lYj_dgNG_sMRyH2EKbpq3OezvK?usp=sharing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgypJ8vY63ZR"
      },
      "source": [
        "# NeRF-- Walk Through ‚õ∑Ô∏è\n",
        "\n",
        "**[Project Page](https://nerfmm.active.vision/) | [Code](https://github.com/ActiveVisionLab/nerfmm) | [Arxiv](https://arxiv.org/abs/2102.07064) | [CoLab Example Data](https://www.robots.ox.ac.uk/~ryan/nerfmm2021/nerfmm_colab_data.tar.gz)**\n",
        "\n",
        "üìñ In this notebook, we will walk you through how we optimise **camera poses**, **intrinsics**, and a **NeRF** at same time, given forward-facing images as the only input, without COLMAP or any SfM/SLAM systems involved. This is a simplified version of our [any_folder target](https://github.com/ActiveVisionLab/nerfmm/tree/main/tasks/any_folder).\n",
        "\n",
        "üèî ***You can try your own data in this colab environment as well (Step 1a)‚ú®.***  \n",
        "\n",
        "ü§î If you have any questions, please raise an issue in our [github repo](https://github.com/ActiveVisionLab/nerfmm)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aUNkfFk9PlL"
      },
      "source": [
        "## Step 0: Set up environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxajrYBS23wg"
      },
      "source": [
        "Clone our repo to use low level functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf17QB803dJK",
        "outputId": "90c01ea5-8980-4d73-eb0c-c39807fc1086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'nerfmm'...\n",
            "remote: Enumerating objects: 100, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 100 (delta 2), reused 1 (delta 0), pack-reused 94 (from 1)\u001b[K\n",
            "Receiving objects: 100% (100/100), 82.80 MiB | 12.05 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ActiveVisionLab/nerfmm.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJqWP2ov3ysp"
      },
      "source": [
        "Get prepared example data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuCLhyaJug0o"
      },
      "outputs": [],
      "source": [
        "!pip install open3d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftbMZNaW3mhg"
      },
      "source": [
        "Import packages and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fbdiiK_i3uIg"
      },
      "outputs": [],
      "source": [
        "import os, random, datetime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import Image\n",
        "from google.colab import files\n",
        "import open3d as o3d\n",
        "from tqdm import tqdm\n",
        "\n",
        "from nerfmm.utils.pose_utils import create_spiral_poses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93WQEGwJ4MrI"
      },
      "source": [
        "Set random seeds for the entire session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qUl5z-Ks4QQb"
      },
      "outputs": [],
      "source": [
        "random.seed(17)\n",
        "np.random.seed(17)\n",
        "torch.manual_seed(17)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJI_GuGZCtON"
      },
      "source": [
        "Import general NeRF-related functions, including:\n",
        "\n",
        "- positional encoding\n",
        "- volume sampling\n",
        "- volume rendering\n",
        "- ray direction computing\n",
        "\n",
        "These functions are useful to general NeRF training, but are not of interest in this notebook so we omit these details. More details can be found in our [git repo](https://https://github.com/ActiveVisionLab/nerfmm).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5ZihlNIzEC36"
      },
      "outputs": [],
      "source": [
        "from nerfmm.utils.pos_enc import encode_position\n",
        "from nerfmm.utils.volume_op import volume_rendering, volume_sampling_ndc\n",
        "from nerfmm.utils.comp_ray_dir import comp_ray_dir_cam_fxfy\n",
        "\n",
        "# utlities\n",
        "from nerfmm.utils.training_utils import mse2psnr\n",
        "from nerfmm.utils.lie_group_helper import convert3x4_4x4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWWR2l9AVaF1"
      },
      "source": [
        "## Step 1a: Load images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCub-nmtxBjR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Define input/output folders\n",
        "input_folder = \"/content/images\"\n",
        "output_folder = \"/content/images_output\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Parameters\n",
        "max_size = 1000  # max height or width after resize\n",
        "\n",
        "# CLAHE for local contrast enhancement\n",
        "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "\n",
        "# Process all image files\n",
        "for fname in os.listdir(input_folder):\n",
        "    if fname.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
        "        fpath = os.path.join(input_folder, fname)\n",
        "        img = cv2.imread(fpath)\n",
        "\n",
        "        # Resize\n",
        "        h, w = img.shape[:2]\n",
        "        scale = max_size / max(h, w)\n",
        "        if scale < 1.0:\n",
        "            img = cv2.resize(img, (int(w * scale), int(h * scale)))\n",
        "\n",
        "        # Convert to grayscale\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Enhance contrast\n",
        "        enhanced = clahe.apply(gray)\n",
        "\n",
        "        # convert to 3-channel grayscale for NeRF-- compatibility\n",
        "        enhanced_3c = cv2.merge([enhanced]*3)\n",
        "\n",
        "        # Save result\n",
        "        out_path = os.path.join(output_folder, fname)\n",
        "        cv2.imwrite(out_path, enhanced_3c)\n",
        "\n",
        "print(\"‚úÖ Preprocessing complete. Images saved to:\", output_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBT2ud4LVgfv"
      },
      "outputs": [],
      "source": [
        "image_dir = '/content/images_output'\n",
        "def load_imgs(image_dir):\n",
        "    img_names = np.array(sorted(os.listdir(image_dir)))  # all image names\n",
        "    img_paths = [os.path.join(image_dir, n) for n in img_names]\n",
        "    N_imgs = len(img_paths)\n",
        "\n",
        "    img_list = []\n",
        "    for p in img_paths:\n",
        "        img = imageio.imread(p)[:, :, :3]  # (H, W, 3) np.uint8\n",
        "        img_list.append(img)\n",
        "    img_list = np.stack(img_list)  # (N, H, W, 3)\n",
        "    img_list = torch.from_numpy(img_list).float() / 255  # (N, H, W, 3) torch.float32\n",
        "    H, W = img_list.shape[1], img_list.shape[2]\n",
        "\n",
        "    results = {\n",
        "        'imgs': img_list,  # (N, H, W, 3) torch.float32\n",
        "        'img_names': img_names,  # (N, )\n",
        "        'N_imgs': N_imgs,\n",
        "        'H': H,\n",
        "        'W': W,\n",
        "    }\n",
        "    return results\n",
        "\n",
        "image_data = load_imgs(image_dir)\n",
        "imgs = image_data['imgs']  # (N, H, W, 3) torch.float32\n",
        "\n",
        "N_IMGS = image_data['N_imgs']\n",
        "H = image_data['H']\n",
        "W = image_data['W']\n",
        "\n",
        "print('Loaded {0} imgs, resolution {1} x {2}'.format(N_IMGS, H, W))\n",
        "print(imgs.shape)\n",
        "plt.imshow(imgs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5Tbg5OLssm0"
      },
      "source": [
        "## Step 2a: Define learnable FOCALS\n",
        "\n",
        "We initialise $f_x = W$ and $f_y = H$.\n",
        "\n",
        "In practice, two coeffcients are initialised at $1.0$ and multiplied with the input image size. We also found a $2^{nd}$-order trick provides slightly better results, but it's not necessary. For more details about the $2^{nd}$-order trick, see the supplementary section in our [arxiv paper](https://https://arxiv.org/abs/2102.07064)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BK6lXxSYrg2m"
      },
      "outputs": [],
      "source": [
        "class LearnFocal(nn.Module):\n",
        "    def __init__(self, H, W, req_grad):\n",
        "        super(LearnFocal, self).__init__()\n",
        "        self.H = H\n",
        "        self.W = W\n",
        "        self.fx = nn.Parameter(torch.tensor(1.0, dtype=torch.float32), requires_grad=req_grad)  # (1, )\n",
        "        self.fy = nn.Parameter(torch.tensor(1.0, dtype=torch.float32), requires_grad=req_grad)  # (1, )\n",
        "\n",
        "    def forward(self):\n",
        "        # order = 2, check our supplementary.\n",
        "        fxfy = torch.stack([self.fx**2 * self.W, self.fy**2 * self.H])\n",
        "        return fxfy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gol8qI-zwQhY"
      },
      "source": [
        "## Step 2b: Define learnable POSES\n",
        "\n",
        "Given $N$ input images, we learn a camera pose for each of them. Camera rotations are optimised in axis-angle representation and translations are optimised in Euclidean space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "or-iL4ocxlI5"
      },
      "outputs": [],
      "source": [
        "def vec2skew(v):\n",
        "    \"\"\"\n",
        "    :param v:  (3, ) torch tensor\n",
        "    :return:   (3, 3)\n",
        "    \"\"\"\n",
        "    zero = torch.zeros(1, dtype=torch.float32, device=v.device)\n",
        "    skew_v0 = torch.cat([ zero,    -v[2:3],   v[1:2]])  # (3, 1)\n",
        "    skew_v1 = torch.cat([ v[2:3],   zero,    -v[0:1]])\n",
        "    skew_v2 = torch.cat([-v[1:2],   v[0:1],   zero])\n",
        "    skew_v = torch.stack([skew_v0, skew_v1, skew_v2], dim=0)  # (3, 3)\n",
        "    return skew_v  # (3, 3)\n",
        "\n",
        "\n",
        "def Exp(r):\n",
        "    \"\"\"so(3) vector to SO(3) matrix\n",
        "    :param r: (3, ) axis-angle, torch tensor\n",
        "    :return:  (3, 3)\n",
        "    \"\"\"\n",
        "    skew_r = vec2skew(r)  # (3, 3)\n",
        "    norm_r = r.norm() + 1e-15\n",
        "    eye = torch.eye(3, dtype=torch.float32, device=r.device)\n",
        "    R = eye + (torch.sin(norm_r) / norm_r) * skew_r + ((1 - torch.cos(norm_r)) / norm_r**2) * (skew_r @ skew_r)\n",
        "    return R\n",
        "\n",
        "\n",
        "def make_c2w(r, t):\n",
        "    \"\"\"\n",
        "    :param r:  (3, ) axis-angle             torch tensor\n",
        "    :param t:  (3, ) translation vector     torch tensor\n",
        "    :return:   (4, 4)\n",
        "    \"\"\"\n",
        "    R = Exp(r)  # (3, 3)\n",
        "    c2w = torch.cat([R, t.unsqueeze(1)], dim=1)  # (3, 4)\n",
        "    c2w = convert3x4_4x4(c2w)  # (4, 4)\n",
        "    return c2w\n",
        "\n",
        "\n",
        "class LearnPose(nn.Module):\n",
        "    def __init__(self, num_cams, learn_R, learn_t):\n",
        "        super(LearnPose, self).__init__()\n",
        "        self.num_cams = num_cams\n",
        "        self.r = nn.Parameter(torch.zeros(size=(num_cams, 3), dtype=torch.float32), requires_grad=learn_R)  # (N, 3)\n",
        "        self.t = nn.Parameter(torch.zeros(size=(num_cams, 3), dtype=torch.float32), requires_grad=learn_t)  # (N, 3)\n",
        "\n",
        "    def forward(self, cam_id):\n",
        "        r = self.r[cam_id]  # (3, ) axis-angle\n",
        "        t = self.t[cam_id]  # (3, )\n",
        "        c2w = make_c2w(r, t)  # (4, 4)\n",
        "        return c2w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7cQhj7M1xgT"
      },
      "source": [
        "## Step 2c: Define a tiny NeRF\n",
        "We define a tiny NeRF for faster colab training, with following modifications:\n",
        "- We use 4 linear layers (official NeRF has 8) before the RGB and density fully connect layers.\n",
        "- We discard the shortcut since the tiny NeRF is quite shallow now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "d1Ahz99U1wyP"
      },
      "outputs": [],
      "source": [
        "class TinyNerf(nn.Module):\n",
        "    def __init__(self, pos_in_dims, dir_in_dims, D):\n",
        "        \"\"\"\n",
        "        :param pos_in_dims: scalar, number of channels of encoded positions\n",
        "        :param dir_in_dims: scalar, number of channels of encoded directions\n",
        "        :param D:           scalar, number of hidden dimensions\n",
        "        \"\"\"\n",
        "        super(TinyNerf, self).__init__()\n",
        "\n",
        "        self.pos_in_dims = pos_in_dims\n",
        "        self.dir_in_dims = dir_in_dims\n",
        "\n",
        "        self.layers0 = nn.Sequential(\n",
        "            nn.Linear(pos_in_dims, D), nn.ReLU(),\n",
        "            nn.Linear(D, D), nn.ReLU(),\n",
        "            nn.Linear(D, D), nn.ReLU(),\n",
        "            nn.Linear(D, D), nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.fc_density = nn.Linear(D, 1)\n",
        "        self.fc_feature = nn.Linear(D, D)\n",
        "        self.rgb_layers = nn.Sequential(nn.Linear(D + dir_in_dims, D//2), nn.ReLU())\n",
        "        self.fc_rgb = nn.Linear(D//2, 3)\n",
        "\n",
        "        self.fc_density.bias.data = torch.tensor([0.1]).float()\n",
        "        self.fc_rgb.bias.data = torch.tensor([0.02, 0.02, 0.02]).float()\n",
        "\n",
        "    def forward(self, pos_enc, dir_enc):\n",
        "        \"\"\"\n",
        "        :param pos_enc: (H, W, N_sample, pos_in_dims) encoded positions\n",
        "        :param dir_enc: (H, W, N_sample, dir_in_dims) encoded directions\n",
        "        :return: rgb_density (H, W, N_sample, 4)\n",
        "        \"\"\"\n",
        "        x = self.layers0(pos_enc)  # (H, W, N_sample, D)\n",
        "        density = self.fc_density(x)  # (H, W, N_sample, 1)\n",
        "\n",
        "        feat = self.fc_feature(x)  # (H, W, N_sample, D)\n",
        "        x = torch.cat([feat, dir_enc], dim=3)  # (H, W, N_sample, D+dir_in_dims)\n",
        "        x = self.rgb_layers(x)  # (H, W, N_sample, D/2)\n",
        "        rgb = self.fc_rgb(x)  # (H, W, N_sample, 3)\n",
        "\n",
        "        rgb_den = torch.cat([rgb, density], dim=3)  # (H, W, N_sample, 4)\n",
        "        return rgb_den"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfcXFPK_4xko"
      },
      "source": [
        "## Step 3: Training pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWkQCW-nqJVn"
      },
      "source": [
        "### Set ray parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "q2ZYvp22fZ9v"
      },
      "outputs": [],
      "source": [
        "class RayParameters():\n",
        "    def __init__(self):\n",
        "      self.NEAR, self.FAR = 0.0, 1.0  # ndc near far\n",
        "      self.N_SAMPLE = 128  # samples per ray\n",
        "      self.POS_ENC_FREQ = 10  # positional encoding freq for location\n",
        "      self.DIR_ENC_FREQ = 4   # positional encoding freq for direction\n",
        "\n",
        "ray_params = RayParameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iTrk6wA2Xce"
      },
      "source": [
        "### Define training function\n",
        "During training, two key steps enable back-propagation:\n",
        "1. Compute ray directions using estimated intrinsics online.\n",
        "2. Sample the 3D volume using estimated poses and intrinsics online.\n",
        "\n",
        "We highlight those two parts in their related comments with \"KEY\" keyword."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AOkynmCt19vJ"
      },
      "outputs": [],
      "source": [
        "def model_render_image(c2w, rays_cam, t_vals, ray_params, H, W, fxfy, nerf_model,\n",
        "                       perturb_t, sigma_noise_std):\n",
        "    \"\"\"\n",
        "    :param c2w:         (4, 4)                  pose to transform ray direction from cam to world.\n",
        "    :param rays_cam:    (someH, someW, 3)       ray directions in camera coordinate, can be random selected\n",
        "                                                rows and cols, or some full rows, or an entire image.\n",
        "    :param t_vals:      (N_samples)             sample depth along a ray.\n",
        "    :param perturb_t:   True/False              perturb t values.\n",
        "    :param sigma_noise_std: float               add noise to raw density predictions (sigma).\n",
        "    :return:            (someH, someW, 3)       volume rendered images for the input rays.\n",
        "    \"\"\"\n",
        "    # KEY 2: sample the 3D volume using estimated poses and intrinsics online.\n",
        "    # (H, W, N_sample, 3), (H, W, 3), (H, W, N_sam)\n",
        "    sample_pos, _, ray_dir_world, t_vals_noisy = volume_sampling_ndc(c2w, rays_cam, t_vals, ray_params.NEAR,\n",
        "                                                                     ray_params.FAR, H, W, fxfy, perturb_t)\n",
        "\n",
        "    # encode position: (H, W, N_sample, (2L+1)*C = 63)\n",
        "    pos_enc = encode_position(sample_pos, levels=ray_params.POS_ENC_FREQ, inc_input=True)\n",
        "\n",
        "    # encode direction: (H, W, N_sample, (2L+1)*C = 27)\n",
        "    ray_dir_world = F.normalize(ray_dir_world, p=2, dim=2)  # (H, W, 3)\n",
        "    dir_enc = encode_position(ray_dir_world, levels=ray_params.DIR_ENC_FREQ, inc_input=True)  # (H, W, 27)\n",
        "    dir_enc = dir_enc.unsqueeze(2).expand(-1, -1, ray_params.N_SAMPLE, -1)  # (H, W, N_sample, 27)\n",
        "\n",
        "    # inference rgb and density using position and direction encoding.\n",
        "    rgb_density = nerf_model(pos_enc, dir_enc)  # (H, W, N_sample, 4)\n",
        "\n",
        "    render_result = volume_rendering(rgb_density, t_vals_noisy, sigma_noise_std, rgb_act_fn=torch.sigmoid)\n",
        "    rgb_rendered = render_result['rgb']  # (H, W, 3)\n",
        "    depth_map = render_result['depth_map']  # (H, W)\n",
        "\n",
        "    result = {\n",
        "        'rgb': rgb_rendered,  # (H, W, 3)\n",
        "        'depth_map': depth_map,  # (H, W)\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def train_one_epoch(imgs, H, W, ray_params, opt_nerf, opt_focal,\n",
        "                    opt_pose, nerf_model, focal_net, pose_param_net):\n",
        "    nerf_model.train()\n",
        "    focal_net.train()\n",
        "    pose_param_net.train()\n",
        "\n",
        "    t_vals = torch.linspace(ray_params.NEAR, ray_params.FAR, ray_params.N_SAMPLE, device='cuda')  # (N_sample,) sample position\n",
        "    L2_loss_epoch = []\n",
        "\n",
        "    # shuffle the training imgs\n",
        "    ids = np.arange(N_IMGS)\n",
        "    np.random.shuffle(ids)\n",
        "\n",
        "    for i in ids:\n",
        "        fxfy = focal_net()\n",
        "\n",
        "        # KEY 1: compute ray directions using estimated intrinsics online.\n",
        "        ray_dir_cam = comp_ray_dir_cam_fxfy(H, W, fxfy[0], fxfy[1])\n",
        "        img = imgs[i].to('cuda')  # (H, W, 4)\n",
        "        c2w = pose_param_net(i)  # (4, 4)\n",
        "\n",
        "        # sample 32x32 pixel on an image and their rays for training.\n",
        "        r_id = torch.randperm(H, device='cuda')[:32]  # (N_select_rows)\n",
        "        c_id = torch.randperm(W, device='cuda')[:32]  # (N_select_cols)\n",
        "        ray_selected_cam = ray_dir_cam[r_id][:, c_id]  # (N_select_rows, N_select_cols, 3)\n",
        "        img_selected = img[r_id][:, c_id]  # (N_select_rows, N_select_cols, 3)\n",
        "\n",
        "        # render an image using selected rays, pose, sample intervals, and the network\n",
        "        render_result = model_render_image(c2w, ray_selected_cam, t_vals, ray_params,\n",
        "                                           H, W, fxfy, nerf_model, perturb_t=True, sigma_noise_std=0.0)\n",
        "        rgb_rendered = render_result['rgb']  # (N_select_rows, N_select_cols, 3)\n",
        "        L2_loss = F.mse_loss(rgb_rendered, img_selected)  # loss for one image\n",
        "\n",
        "        L2_loss.backward()\n",
        "        opt_nerf.step()\n",
        "        opt_focal.step()\n",
        "        opt_pose.step()\n",
        "        opt_nerf.zero_grad()\n",
        "        opt_focal.zero_grad()\n",
        "        opt_pose.zero_grad()\n",
        "\n",
        "        L2_loss_epoch.append(L2_loss)\n",
        "\n",
        "    L2_loss_epoch_mean = torch.stack(L2_loss_epoch).mean().item()\n",
        "    return L2_loss_epoch_mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBPABkie6fTU"
      },
      "source": [
        "### Define evaluation function\n",
        "\n",
        "We render an image from a $4\\times4$ identity matrix as we do not have train/eval pose at all.\n",
        "\n",
        "Render results can be found in the tensorboard above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BiTLHOUt6eud"
      },
      "outputs": [],
      "source": [
        "def render_novel_view(c2w, H, W, fxfy, ray_params, nerf_model):\n",
        "    nerf_model.eval()\n",
        "\n",
        "    ray_dir_cam = comp_ray_dir_cam_fxfy(H, W, fxfy[0], fxfy[1])\n",
        "    t_vals = torch.linspace(ray_params.NEAR, ray_params.FAR, ray_params.N_SAMPLE, device='cuda')  # (N_sample,) sample position\n",
        "\n",
        "    c2w = c2w.to('cuda')  # (4, 4)\n",
        "\n",
        "    # split an image to rows when the input image resolution is high\n",
        "    rays_dir_cam_split_rows = ray_dir_cam.split(10, dim=0)  # input 10 rows each time\n",
        "    rendered_img = []\n",
        "    rendered_depth = []\n",
        "    for rays_dir_rows in rays_dir_cam_split_rows:\n",
        "        render_result = model_render_image(c2w, rays_dir_rows, t_vals, ray_params,\n",
        "                                           H, W, fxfy, nerf_model,\n",
        "                                           perturb_t=False, sigma_noise_std=0.0)\n",
        "        rgb_rendered_rows = render_result['rgb']  # (num_rows_eval_img, W, 3)\n",
        "        depth_map = render_result['depth_map']  # (num_rows_eval_img, W)\n",
        "\n",
        "        rendered_img.append(rgb_rendered_rows)\n",
        "        rendered_depth.append(depth_map)\n",
        "\n",
        "    # combine rows to an image\n",
        "    rendered_img = torch.cat(rendered_img, dim=0)  # (H, W, 3)\n",
        "    rendered_depth = torch.cat(rendered_depth, dim=0)  # (H, W)\n",
        "    return rendered_img, rendered_depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzzITg_05yYc"
      },
      "source": [
        "## Tensorborad üìà"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouF4Q0fg53b2"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEJJtN7A6ZGp"
      },
      "source": [
        "## Step 4: Actual training ‚öΩ\n",
        "- In our paper, we train our our NeRF-- for 10K epochs. but due to the limited GPU resources available in colab, we train our NeRF-- system for 200 epochs (< 10 min) for now, from which a blurry nerf and a rough estimation of camera parameters can be acquired.\n",
        "- Depending on scenes, the depth estimation should start making sense at some point. All examples in this notebook should have a correct depth estimation before 200 epochs. However, we cannot guarentee similar performance for uploaded images.\n",
        "- ***Despite the renderings after 200 epochs are blurry, which can be improved progressively during training, a near-far relationship should be established in 200 epochs for our prepared data in this notebook experiment***. A darker area in a depth map denotes closer distance than a brighter area.\n",
        "- During training, we check if the joint optimisation is successful by writing the depth estimation to tensorboard every 50 epochs. The depth map is estimated from a camera pose that is set to identity matrix. Check the *Image* tab in the tensorboard above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8XVprYkA0XY"
      },
      "outputs": [],
      "source": [
        "scene_name = \"images_output\" # Preprocessed Image path\n",
        "N_EPOCH = 300  # set to 1000 to get slightly better results. we use 10K epoch in our paper.\n",
        "EVAL_INTERVAL = 50  # render an image to visualise for every this interval.\n",
        "\n",
        "# Initialise all trainabled parameters\n",
        "focal_net = LearnFocal(H, W, req_grad=True).cuda()\n",
        "pose_param_net = LearnPose(num_cams=N_IMGS, learn_R=True, learn_t=True).cuda()\n",
        "\n",
        "# Get a tiny NeRF model. Hidden dimension set to 128\n",
        "nerf_model = TinyNerf(pos_in_dims=63, dir_in_dims=27, D=128).cuda()\n",
        "\n",
        "# Set lr and scheduler: these are just stair-case exponantial decay lr schedulers.\n",
        "opt_nerf = torch.optim.Adam(nerf_model.parameters(), lr=0.001)\n",
        "opt_focal = torch.optim.Adam(focal_net.parameters(), lr=0.001)\n",
        "opt_pose = torch.optim.Adam(pose_param_net.parameters(), lr=0.001)\n",
        "\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "scheduler_nerf = MultiStepLR(opt_nerf, milestones=list(range(0, 10000, 10)), gamma=0.9954)\n",
        "scheduler_focal = MultiStepLR(opt_focal, milestones=list(range(0, 10000, 100)), gamma=0.9)\n",
        "scheduler_pose = MultiStepLR(opt_pose, milestones=list(range(0, 10000, 100)), gamma=0.9)\n",
        "\n",
        "# Set tensorboard writer\n",
        "writer = SummaryWriter(log_dir=os.path.join('logs', scene_name, str(datetime.datetime.now().strftime('%y%m%d_%H%M%S'))))\n",
        "\n",
        "# Store poses to visualise them later\n",
        "pose_history = []\n",
        "\n",
        "# Training\n",
        "print('Training... Check results in the tensorboard above.')\n",
        "for epoch_i in tqdm(range(N_EPOCH), desc='Training'):\n",
        "    L2_loss = train_one_epoch(imgs, H, W, ray_params, opt_nerf, opt_focal,\n",
        "                              opt_pose, nerf_model, focal_net, pose_param_net)\n",
        "    train_psnr = mse2psnr(L2_loss)\n",
        "\n",
        "    writer.add_scalar('train/psnr', train_psnr, epoch_i)\n",
        "\n",
        "    fxfy = focal_net()\n",
        "    print('epoch {0:4d} Training PSNR {1:.3f}, estimated fx {2:.1f} fy {3:.1f}'.format(epoch_i, train_psnr, fxfy[0], fxfy[1]))\n",
        "\n",
        "    scheduler_nerf.step()\n",
        "    scheduler_focal.step()\n",
        "    scheduler_pose.step()\n",
        "\n",
        "    learned_c2ws = torch.stack([pose_param_net(i) for i in range(N_IMGS)])  # (N, 4, 4)\n",
        "    pose_history.append(learned_c2ws[:, :3, 3])  # (N, 3) only store positions as we vis in 2D.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if (epoch_i+1) % EVAL_INTERVAL == 0:\n",
        "            eval_c2w = torch.eye(4, dtype=torch.float32)  # (4, 4)\n",
        "            fxfy = focal_net()\n",
        "            rendered_img, rendered_depth = render_novel_view(eval_c2w, H, W, fxfy, ray_params, nerf_model)\n",
        "            writer.add_image('eval/img', rendered_img.permute(2, 0, 1), global_step=epoch_i)\n",
        "            writer.add_image('eval/depth', rendered_depth.unsqueeze(0), global_step=epoch_i)\n",
        "\n",
        "pose_history = torch.stack(pose_history).detach().cpu().numpy()  # (N_epoch, N_img, 3)\n",
        "print('Training finished.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LLCPGAatJXH"
      },
      "source": [
        "## Step 5: Novel View Synthesis üñºÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fkZ2jWWPpWX2"
      },
      "outputs": [],
      "source": [
        "def depth_to_point_cloud(depth_map, color_map, fxfy, c2w):\n",
        "    \"\"\"\n",
        "    Convert depth map to 3D point cloud\n",
        "\n",
        "    Args:\n",
        "        depth_map: (H, W) tensor of depth values\n",
        "        color_map: (H, W, 3) tensor of RGB values\n",
        "        fxfy: focal length\n",
        "        c2w: camera-to-world transformation matrix\n",
        "\n",
        "    Returns:\n",
        "        points: (N, 3) tensor of 3D points\n",
        "        colors: (N, 3) tensor of RGB colors\n",
        "    \"\"\"\n",
        "    H, W = depth_map.shape\n",
        "    device = depth_map.device  # Get the device from the input tensor\n",
        "\n",
        "    print(device)\n",
        "    # Ensure all inputs are on the same device\n",
        "    fxfy = fxfy.to(device)\n",
        "    c2w = c2w.to(device)\n",
        "    color_map = color_map.to(device)\n",
        "\n",
        "    # Create pixel coordinates grid on the same device as depth_map\n",
        "    i, j = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing='ij')\n",
        "    i = i.flatten().float()\n",
        "    j = j.flatten().float()\n",
        "\n",
        "    # Convert to NDC coordinates\n",
        "    x = (j - W/2) / fxfy[0]\n",
        "    y = (i - H/2) / fxfy[1]\n",
        "\n",
        "    # Get depth values\n",
        "    z = depth_map.flatten()\n",
        "\n",
        "    # Create camera coordinate points\n",
        "    points_cam = torch.stack([x * z, y * z, z], dim=-1)  # (H*W, 3)\n",
        "\n",
        "    # Convert to world coordinates\n",
        "    R = c2w[:3, :3]  # (3, 3)\n",
        "    t = c2w[:3, 3:4]  # (3, 1)\n",
        "\n",
        "    points_world = torch.matmul(points_cam, R.T) + t.T  # (H*W, 3)\n",
        "\n",
        "    # Get colors\n",
        "    colors = color_map.reshape(-1, 3)  # (H*W, 3)\n",
        "\n",
        "    # Filter out points with invalid depth\n",
        "    valid_mask = ~torch.isnan(z) & (z > 0)\n",
        "    points_world = points_world[valid_mask]\n",
        "    colors = colors[valid_mask]\n",
        "\n",
        "    return points_world, colors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "M5W1Z5tQw5xY"
      },
      "outputs": [],
      "source": [
        "def create_pointcloud(all_points, all_colors, scene_name, sample_ratio=0.1):\n",
        "    \"\"\"\n",
        "    Create 3D visualization of the scene point cloud\n",
        "\n",
        "    Args:\n",
        "        all_points: list of point cloud tensors\n",
        "        all_colors: list of color tensors\n",
        "        scene_name: name of the scene\n",
        "        sample_ratio: ratio of points to sample (to reduce size)\n",
        "    \"\"\"\n",
        "    print(\"Creating pointcloud...\")\n",
        "\n",
        "    # Combine all points and colors (move to CPU first)\n",
        "    points = torch.cat([p.cpu() for p in all_points], dim=0).numpy()\n",
        "    colors = torch.cat([c.cpu() for c in all_colors], dim=0).numpy()\n",
        "\n",
        "    # Sample points to reduce size\n",
        "    n_points = points.shape[0]\n",
        "    sample_indices = np.random.choice(n_points, size=int(n_points * sample_ratio), replace=False)\n",
        "    points = points[sample_indices]\n",
        "    colors = colors[sample_indices]\n",
        "\n",
        "    # Create Open3D point cloud\n",
        "    pcd = o3d.geometry.PointCloud()\n",
        "    pcd.points = o3d.utility.Vector3dVector(points)\n",
        "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
        "\n",
        "    # Estimate normals for better visualization\n",
        "    pcd.estimate_normals()\n",
        "    pcd.orient_normals_consistent_tangent_plane(k=30)\n",
        "\n",
        "    # Save the point cloud\n",
        "    os.makedirs('nvs_results', exist_ok=True)\n",
        "    o3d.io.write_point_cloud(os.path.join('nvs_results', scene_name + '_pointcloud.ply'), pcd)\n",
        "\n",
        "    num_points = len(pcd.points)\n",
        "    print(\"Number of points in the point cloud:\", num_points)\n",
        "\n",
        "    print(f\"Point cloud saved to nvs_results/{scene_name}_pointcloud.ply\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BzAPCQRZo98c"
      },
      "outputs": [],
      "source": [
        "# Render full images are time consuming, especially on colab so we render a smaller version instead.\n",
        "resize_ratio = 4\n",
        "with torch.no_grad():\n",
        "    optimised_poses = torch.stack([pose_param_net(i) for i in range(N_IMGS)])\n",
        "    radii = np.percentile(np.abs(optimised_poses.cpu().numpy()[:, :3, 3]), q=50, axis=0)  # (3,)\n",
        "    spiral_c2ws = create_spiral_poses(radii, focus_depth=3.5, n_poses=30, n_circle=1)\n",
        "    spiral_c2ws = torch.from_numpy(spiral_c2ws).float()  # (N, 3, 4)\n",
        "\n",
        "    # change intrinsics according to resize ratio\n",
        "    fxfy = focal_net()\n",
        "    novel_fxfy = fxfy / resize_ratio\n",
        "    novel_H, novel_W = H // resize_ratio, W // resize_ratio\n",
        "\n",
        "    print('NeRF trained in {0:d} x {1:d} for {2:d} epochs'.format(H, W, N_EPOCH))\n",
        "    print('Rendering novel views in {0:d} x {1:d}'.format(novel_H, novel_W))\n",
        "\n",
        "    novel_img_list, novel_depth_list = [], []\n",
        "    # For storing point cloud data\n",
        "    all_points = []\n",
        "    all_colors = []\n",
        "\n",
        "    for i in tqdm(range(spiral_c2ws.shape[0]), desc='novel view rendering'):\n",
        "        novel_img, novel_depth = render_novel_view(spiral_c2ws[i], novel_H, novel_W, novel_fxfy, ray_params, nerf_model)\n",
        "        novel_img_list.append(novel_img)\n",
        "        novel_depth_list.append(novel_depth)\n",
        "\n",
        "        # Generate point cloud for this view\n",
        "        points, colors = depth_to_point_cloud(novel_depth, novel_img, novel_fxfy, spiral_c2ws[i])\n",
        "        all_points.append(points)\n",
        "        all_colors.append(colors)\n",
        "\n",
        "    print('Novel view rendering done. Saving to GIF images...')\n",
        "    novel_img_list = (torch.stack(novel_img_list) * 255).cpu().numpy().astype(np.uint8)\n",
        "    novel_depth_list = (torch.stack(novel_depth_list) * 200).cpu().numpy().astype(np.uint8)  # depth is always in 0 to 1 in NDC\n",
        "\n",
        "    os.makedirs('nvs_results', exist_ok=True)\n",
        "    imageio.mimwrite(os.path.join('nvs_results', scene_name + '_img.gif'), novel_img_list, fps=30)\n",
        "    imageio.mimwrite(os.path.join('nvs_results', scene_name + '_depth.gif'), novel_depth_list, fps=30)\n",
        "    print('GIF images saved.')\n",
        "\n",
        "    create_pointcloud(all_points, all_colors, scene_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfycKI0OBMnG"
      },
      "outputs": [],
      "source": [
        "# Render novel views from a sprial camera trajectory.\n",
        "# The spiral trajectory generation function is modified from https://github.com/kwea123/nerf_pl.\n",
        "from nerfmm.utils.pose_utils import create_spiral_poses\n",
        "\n",
        "# Render full images are time consuming, especially on colab so we render a smaller version instead.\n",
        "resize_ratio = 4\n",
        "with torch.no_grad():\n",
        "    optimised_poses = torch.stack([pose_param_net(i) for i in range(N_IMGS)])\n",
        "    radii = np.percentile(np.abs(optimised_poses.cpu().numpy()[:, :3, 3]), q=50, axis=0)  # (3,)\n",
        "    spiral_c2ws = create_spiral_poses(radii, focus_depth=3.5, n_poses=30, n_circle=1)\n",
        "    spiral_c2ws = torch.from_numpy(spiral_c2ws).float()  # (N, 3, 4)\n",
        "\n",
        "    # change intrinsics according to resize ratio\n",
        "    fxfy = focal_net()\n",
        "    novel_fxfy = fxfy / resize_ratio\n",
        "    novel_H, novel_W = H // resize_ratio, W // resize_ratio\n",
        "\n",
        "    print('NeRF trained in {0:d} x {1:d} for {2:d} epochs'.format(H, W, N_EPOCH))\n",
        "    print('Rendering novel views in {0:d} x {1:d}'.format(novel_H, novel_W))\n",
        "\n",
        "    novel_img_list, novel_depth_list = [], []\n",
        "    for i in tqdm(range(spiral_c2ws.shape[0]), desc='novel view rendering'):\n",
        "        novel_img, novel_depth = render_novel_view(spiral_c2ws[i], novel_H, novel_W, novel_fxfy,\n",
        "                                                   ray_params, nerf_model)\n",
        "        novel_img_list.append(novel_img)\n",
        "        novel_depth_list.append(novel_depth)\n",
        "\n",
        "    print('Novel view rendering done. Saving to GIF images...')\n",
        "    novel_img_list = (torch.stack(novel_img_list) * 255).cpu().numpy().astype(np.uint8)\n",
        "    novel_depth_list = (torch.stack(novel_depth_list) * 200).cpu().numpy().astype(np.uint8)  # depth is always in 0 to 1 in NDC\n",
        "\n",
        "    os.makedirs('nvs_results', exist_ok=True)\n",
        "    imageio.mimwrite(os.path.join('nvs_results', scene_name + '_img.gif'), novel_img_list, fps=30)\n",
        "    imageio.mimwrite(os.path.join('nvs_results', scene_name + '_depth.gif'), novel_depth_list, fps=30)\n",
        "    print('GIF images saved.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpipOlC9vLwJ"
      },
      "outputs": [],
      "source": [
        "Image(open(os.path.join('nvs_results', scene_name + '_img.gif'), 'rb').read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7PdJJujwctv"
      },
      "outputs": [],
      "source": [
        "Image(open(os.path.join('nvs_results', scene_name + '_depth.gif'), 'rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_L7Ax57Gk07"
      },
      "source": [
        "## Step 6: Visualise Camera Poses üì∑\n",
        "\n",
        "We visualise the optimisation process of camera poses from Z direction, i.e. on 2D XY-plane."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGDrziB3IjQu"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "ax0 = plt.subplot(111)\n",
        "ax0.set_xlim((-0.5, 0.5))\n",
        "ax0.set_ylim((-0.5, 0.5))\n",
        "ax0.set_xlabel('X')\n",
        "ax0.set_ylabel('Y')\n",
        "ax0.grid(ls='--', color='0.7')\n",
        "\n",
        "title = ax0.set_title('')\n",
        "traj_line, = ax0.plot([], [], c='blue', ls='-', marker='^', linewidth=0.7, markersize=4)\n",
        "\n",
        "def drawframe(fr_id):\n",
        "    \"\"\"\n",
        "    :param fr_id: frame id\n",
        "    :param poses: (N_img, 3) camera positions\n",
        "    \"\"\"\n",
        "    poses = pose_history[fr_id]\n",
        "    traj_line.set_data(poses[:, 0], poses[:, 1])\n",
        "    title.set_text('epoch {0:4d}'.format(fr_id))\n",
        "    return (traj_line,)\n",
        "\n",
        "anim = animation.FuncAnimation(fig, drawframe, frames=N_EPOCH, interval=100)\n",
        "\n",
        "plt.close(anim._fig)\n",
        "HTML(anim.to_html5_video())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
